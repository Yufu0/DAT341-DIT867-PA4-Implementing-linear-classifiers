{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "\n",
    "Celio Bueri, Christoph Stelz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise question\n",
    "\n",
    "Consider the following visualization of the two training sets:\n",
    "\n",
    "![](figures/exercise_question.svg)\n",
    "\n",
    "It is easy to see that the first training data set can be separated with a line in the upper right corner, i.e. the set is linear separable.\n",
    "\n",
    "In the second example, the data are not linearly separable. But because our Perceptron is a linear classifier with binary inputs and outputs, it is unable to fit to this scenario appropriately.\n",
    "\n",
    "In this case, we would have to hand-engineer an additional feature for the classifier to be able to fit. For example, instead of the month we could consider a property like `is_summer` derived from the month and location on the northern or southern hemisphere."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code\n",
    "\n",
    "This is the base class for our linear classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Implementation\n",
    "\n",
    "Here we implement the SVC training algorithm found in the paper by Shalev-Shwartz et al.\n",
    "\n",
    "Parts of the class are adapted from the Perceptron example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SVCImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_iteration_loss=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many samples should be considered for training.\n",
    "        If print_iteration_loss is set to False, no updates on the training\n",
    "        process will be printed.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_iteration_loss = print_iteration_loss\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the Pegasos learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # initialise the number of iteration\n",
    "        t = 0\n",
    "\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # update the weights\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * y * x\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w\n",
    "\n",
    "            # print every 1000 iterations\n",
    "            if self.print_iteration_loss and t % 1000 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_iteration_loss=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_iteration_loss = print_iteration_loss\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "       # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # initialise the number of iteration\n",
    "        t = 0\n",
    "\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # update the weights\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * x * y / (1 + np.exp(y*score))\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w\n",
    "\n",
    "            # print every 1000 iterations\n",
    "            if self.print_iteration_loss and t % 1000 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for logistic regression\n",
    "            loss += np.log(1 + np.exp(-y * x.dot(self.w)))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing training data\n",
    "\n",
    "Here we load our data from the textfile, use 80% of it for training and 20% for testing our classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our training dataset contains 9531 samples.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our training dataset contains {len(Xtrain)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Testing our classifiers\n",
    "\n",
    "We create a pipeline with a TfidfVectorizer and use only the 1000 best features. Then we train our SVC on $10\\,000$ samples with a regularizer value of $0.0001$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1000, train_loss : 1.8570092042995647\n",
      "iteration : 2000, train_loss : 1.0675309594121538\n",
      "iteration : 3000, train_loss : 0.7837818830556673\n",
      "iteration : 4000, train_loss : 0.6604807235583925\n",
      "iteration : 5000, train_loss : 0.587847304163608\n",
      "iteration : 6000, train_loss : 0.5375205023786316\n",
      "iteration : 7000, train_loss : 0.5125643019352591\n",
      "iteration : 8000, train_loss : 0.4920794052667724\n",
      "iteration : 9000, train_loss : 0.5166901129363458\n",
      "iteration : 10000, train_loss : 0.45465316763362496\n",
      "Training time: 1.65 sec.\n",
      "Accuracy: 0.8091.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(n_iter=10000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1000, train_loss : 1.3169432572978876\n",
      "iteration : 2000, train_loss : 0.6322705321395731\n",
      "iteration : 3000, train_loss : 0.5262734438194467\n",
      "iteration : 4000, train_loss : 0.4870529222015718\n",
      "iteration : 5000, train_loss : 0.452090813648628\n",
      "iteration : 6000, train_loss : 0.4396097492638769\n",
      "iteration : 7000, train_loss : 0.44485574482690793\n",
      "iteration : 8000, train_loss : 0.4572865894544195\n",
      "iteration : 9000, train_loss : 0.4360114935474195\n",
      "iteration : 10000, train_loss : 0.430937980836665\n",
      "Training time: 1.71 sec.\n",
      "Accuracy: 0.8154.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    LogisticRegressionImpl(n_iter=10000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the SVC yields an accuracy of 82% and the logistic regression performs slightly worse with an accuracy of around 81%.\n",
    "\n",
    "However, this is with guesstimated hyperparameters and leaves some accuracy on the table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameter Optimization\n",
    "\n",
    "We perform a grid search and optimize the following hyperparameters:\n",
    "\n",
    "- iteration count set to ten thousand, three thousand, hundred thousand and one million\n",
    "- regularizer set to $10^{-2}$, …, $10^{-5}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_iter': [10000, 30000, 100000, 1000000],\n",
    "    'classifier__regularizer': [0.01, 0.001, 0.0001, 0.00001]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (loss=-0.426):\n",
      "{'classifier__n_iter': 1000000, 'classifier__regularizer': 1e-05}\n",
      "Accuracy: 0.8871.\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[\n",
    "    (\"vect\",TfidfVectorizer()),\n",
    "    (\"select\", SelectKBest(k=1000)),\n",
    "    (\"norm\", Normalizer()),\n",
    "    (\"classifier\", SVCImpl(print_iteration_loss=False))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameter (loss=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "Yguess = grid_search.best_estimator_.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properly tuning the hyperparameters resulted in a huge performance gain. The best estimator iterates over the training set around 100 times and uses a really small regularizer term.\n",
    "\n",
    "This allows it to get an accuracy of around 88.5% on the test set!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (loss=-0.405):\n",
      "{'classifier__n_iter': 1000000, 'classifier__regularizer': 1e-05}\n",
      "Accuracy: 0.8808.\n"
     ]
    }
   ],
   "source": [
    "log_pipeline = Pipeline(steps=[\n",
    "    (\"vect\",TfidfVectorizer()),\n",
    "    (\"select\", SelectKBest(k=1000)),\n",
    "    (\"norm\", Normalizer()),\n",
    "    (\"classifier\", LogisticRegressionImpl(print_iteration_loss=False))\n",
    "])\n",
    "\n",
    "log_grid_search = GridSearchCV(log_pipeline, param_grid, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "log_grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameter (loss=%0.3f):\" % log_grid_search.best_score_)\n",
    "print(log_grid_search.best_params_)\n",
    "\n",
    "log_Yguess = log_grid_search.best_estimator_.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, log_Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the tuning, our logistic regression is almost at the same level of accuracy as the SVC. Interestingly, it chose the same hyper parameters. In future research, it might be interesting to train with more iterations and explore the regularizer value space more thoroughly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "\n",
    "### a) BLAS operations\n",
    "\n",
    "In this version we replaced the Numpy operations with BLAS functions from scipy.\n",
    "One pitfall we encountered is that with inplace operations like `daxpy`, the argument ordering matters a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy\n",
    "\n",
    "class SVCImplBLAS(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_iteration_loss=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_iteration_loss = print_iteration_loss\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the pegasos learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "        \n",
    "        t = 0\n",
    "        # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # training algorithm:\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = ddot(self.w, x)\n",
    "\n",
    "            # since in either case we are going to scale the weight vector\n",
    "            # by this term, we can pull it outside the if-clause\n",
    "            dscal(1 - self.regularizer * lr, self.w)\n",
    "\n",
    "\n",
    "            if y * score < 1:\n",
    "                # in this case, we also need to add to the weight vector\n",
    "                # note the order of the arguments: w += a * x\n",
    "                daxpy(x, self.w, a=(lr * y))\n",
    "                \n",
    "            if self.print_iteration_loss and t % 1000 == 0:\n",
    "                print(f'iteration {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(ddot(self.w, self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * ddot(x, self.w))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### b) Sparse Vectors\n",
    "\n",
    "Here we only store values that are non-zero. We do this by keeping track of their inidices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class SVCImplSparse(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_iteration_loss=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_iteration_loss = print_iteration_loss\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # zip entries together and cache them to speed up the iteration\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            # deconstruct the training sample\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = np.dot(self.w[x.indices], x.data)\n",
    "\n",
    "            # regularizer\n",
    "            self.w *= (1 - self.regularizer * lr)\n",
    "            \n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                self.w[x.indices] += (lr * y) * x.data\n",
    "\n",
    "            if self.print_iteration_loss and t % 1000 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2.0 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### c) Faster scaling\n",
    "\n",
    "In addition to the sparse version, we extract a scaling factor that can be updated faster than a whole vector. The scaling factor is then multiplied into the vector at the very end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w, a):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return a * np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SVCImplSparseScaling(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_iteration_loss=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_iteration_loss = print_iteration_loss\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        # pick indices randomly\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # list of instances picked randomly\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        # initialize vector scaling\n",
    "        a = 1\n",
    "        #  number of iterations\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "            # update learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = sparse_dense_dot(x, self.w, a)\n",
    "\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                add_sparse_to_dense(x, self.w, (lr * y / a))\n",
    "\n",
    "            # update vector scaling\n",
    "            # We verify that a is positive because for large number of iteration\n",
    "            # a could be rounded as 0.0 and cause an error\n",
    "            if (1 - self.regularizer * lr) * a > 0.0:\n",
    "                a = (1 - self.regularizer * lr) * a\n",
    "\n",
    "        self.w =  a * self.w\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation\n",
    "\n",
    "We no evaluate the performance of our different estimators.\n",
    "We do this by creating a fitting the pipeline a number of times. Also, we keep track of the accuracy to check that we still get correct results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline):\n",
    "    #pipeline.fit(Xtrain, Ytrain) # make sure its fitted\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline\n",
    "\n",
    "First we evaluate our baseline version, which is our initial implementation from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_baseline_classifier():\n",
    "    np.random.seed(42)\n",
    "    pipeline_baseline = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        SVCImpl(n_iter=100000,regularizer=0.0001,print_iteration_loss=False)\n",
    "    )\n",
    "    pipeline_baseline.fit(Xtrain, Ytrain)\n",
    "    return pipeline_baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.96 s ± 113 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = create_baseline_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8368.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(create_baseline_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS\n",
    "\n",
    "Next comes the BLAS version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_blas_classifier():\n",
    "    np.random.seed(42) # fix the seed so the two algorithms see the same samples\n",
    "    pipeline_blas = make_pipeline(\n",
    "        TfidfVectorizer(),\n",
    "        SelectKBest(k=1000),\n",
    "        Normalizer(),\n",
    "        SVCImplBLAS(n_iter=100000,regularizer=0.0001,print_iteration_loss=False)\n",
    "    )\n",
    "    pipeline_blas.fit(Xtrain, Ytrain)\n",
    "    return pipeline_blas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8 s ± 79.9 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "_ = create_blas_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8368.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(create_blas_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the BLAS optimization yields us faster training times at no expense to the accuracy. The standard deviation on the benchmark runs seems to indicate that the difference is statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "b = ax.bar([1,2], [1.96, 1.8], yerr=[113e-3, 79.9e-3], width=0.8)\n",
    "ax.bar_label(b, labels=['Baseline', 'BLAS'], padding=5)\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sparse vector\n",
    "\n",
    "Now we will try our sparse vector variant.\n",
    "Note that in the pipeline below, we omit the `SelectKBest` step and train on all features. This makes a performance comparison to the baseline version unreasonable.\n",
    "\n",
    "Furthermore, we increased the `ngram_range` to $[1,2]$, meaning that the feature space explodes even further. When we try to ran this with our baseline version, we got an out-of-memory exception because the weight matrix would exceed our main memory size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_sparse_classifier():\n",
    "    np.random.seed(42)\n",
    "    pipeline_sparse = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)),\n",
    "        Normalizer(),\n",
    "        SVCImplSparse(n_iter=100000,regularizer=0.0001,print_iteration_loss=False)\n",
    "    )\n",
    "    pipeline_sparse.fit(Xtrain, Ytrain)\n",
    "    return pipeline_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n1 -r1\n",
    "_ = make_sparse_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8519.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(make_sparse_classifier())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the increased feature count, we exceed 85% accuracy! That's nice. However, the operation is a bit slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Speeding up the scaling operation\n",
    "\n",
    "Finally, we run the optimization of the scaling factor with identical parameters to the above sparse version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_scaling_classifier():\n",
    "    np.random.seed(42)\n",
    "    pipeline_sparse_scaling = make_pipeline(\n",
    "        TfidfVectorizer(ngram_range=(1,2)),\n",
    "        Normalizer(),\n",
    "        SVCImplSparseScaling(n_iter=100000,regularizer=0.0001,print_iteration_loss=False)\n",
    "    )\n",
    "    pipeline_sparse_scaling.fit(Xtrain, Ytrain)\n",
    "    return pipeline_sparse_scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.72 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 1\n",
    "_ = make_scaling_classifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8561.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(make_scaling_classifier())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "b = ax.bar([1,2], [38, 8.72], width=0.8)\n",
    "ax.bar_label(b, labels=['Sparse', 'Improved scaling'], padding=5)\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the scaling mechanism wildly improves our training performance without lowering the accuracy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
