{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "\n",
    "Celio Bueri, Christoph Stelz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise question\n",
    "\n",
    "In the second example, the data are not linearly separable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SVCImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = x.dot(self.w)\n",
    "\n",
    "                if y * score < 1:\n",
    "                    self.w = (1 - self.regularizer * self.lr) * self.w + self.lr * y * x\n",
    "                else:\n",
    "                    self.w = (1 - self.regularizer * self.lr) * self.w\n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, probability=False, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.probability = probability\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "       # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                score = y * x.dot(self.w)\n",
    "                self.w = (1 - self.regularizer * self.lr) * self.w + self.lr * y * self.sigmoid(-score) * x\n",
    "\n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for logistic regression\n",
    "            loss += float(np.log(1 + np.exp(-y * x.dot(self.w))))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our classifiers \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 0.7057455996879557\n",
      "epoch : 1, train_loss : 0.5611505980919427\n",
      "epoch : 2, train_loss : 0.5039397829488059\n",
      "epoch : 3, train_loss : 0.4696387462877015\n",
      "epoch : 4, train_loss : 0.4462894936516433\n",
      "epoch : 5, train_loss : 0.42830267475548656\n",
      "epoch : 6, train_loss : 0.4143419948195836\n",
      "epoch : 7, train_loss : 0.40286881739702346\n",
      "epoch : 8, train_loss : 0.39358083068490124\n",
      "epoch : 9, train_loss : 0.3857290503206957\n",
      "epoch : 10, train_loss : 0.37876299806577396\n",
      "epoch : 11, train_loss : 0.3726294761723526\n",
      "epoch : 12, train_loss : 0.3671505317718635\n",
      "epoch : 13, train_loss : 0.3621148454079944\n",
      "epoch : 14, train_loss : 0.3574758489887846\n",
      "epoch : 15, train_loss : 0.3532854985167091\n",
      "epoch : 16, train_loss : 0.34951368393329507\n",
      "epoch : 17, train_loss : 0.346134213194824\n",
      "epoch : 18, train_loss : 0.3430387742790991\n",
      "epoch : 19, train_loss : 0.340125666100796\n",
      "Training time: 4.74 sec.\n",
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our LogisticRegression implementation\n",
    "    SVCImpl()\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 0.6288857169774531\n",
      "epoch : 1, train_loss : 0.5840018940929901\n",
      "epoch : 2, train_loss : 0.5511767701367489\n",
      "epoch : 3, train_loss : 0.5261244113878396\n",
      "epoch : 4, train_loss : 0.5062936920256184\n",
      "epoch : 5, train_loss : 0.49011851663793615\n",
      "epoch : 6, train_loss : 0.4765995710252395\n",
      "epoch : 7, train_loss : 0.4650742894372502\n",
      "epoch : 8, train_loss : 0.45508760073903537\n",
      "epoch : 9, train_loss : 0.4463169192656121\n",
      "epoch : 10, train_loss : 0.4385270914760717\n",
      "epoch : 11, train_loss : 0.43154242018454747\n",
      "epoch : 12, train_loss : 0.4252287470781953\n",
      "epoch : 13, train_loss : 0.41948164728788945\n",
      "epoch : 14, train_loss : 0.4142184477857359\n",
      "epoch : 15, train_loss : 0.4093727030957251\n",
      "epoch : 16, train_loss : 0.4048902894709579\n",
      "epoch : 17, train_loss : 0.40072658932183824\n",
      "epoch : 18, train_loss : 0.3968444253813822\n",
      "epoch : 19, train_loss : 0.3932125202979789\n",
      "Training time: 4.23 sec.\n",
      "Accuracy: 0.8175.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our LogisticRegression implementation\n",
    "    LogisticRegressionImpl()\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_iter': [10, 30, 10],\n",
    "    'classifier__regularizer': [0.1, 1, 2]\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"vect\",TfidfVectorizer()),\n",
    "    (\"select\", SelectKBest(k=1000)),\n",
    "    (\"norm\", Normalizer()),\n",
    "    (\"classifier\", SVCImpl(print_epoch_stats=False))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameter (loss=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "\n",
    "### a) BLAS operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy\n",
    "\n",
    "class SVCImplBLAS(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = ddot(self.w, x)\n",
    "                \n",
    "                dscal(1 - self.regularizer * self.lr, self.w)\n",
    "                if y * score < 1:\n",
    "                    daxpy(x, self.w, a=(self.lr * y))\n",
    "                \n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(ddot(self.w, self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline):\n",
    "    pipeline.fit(Xtrain, Ytrain) # make sure its fitted\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.52 s ± 134 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_baseline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(print_epoch_stats=False)\n",
    ")\n",
    "pipeline_baseline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13 s ± 94.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pipeline_blas = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImplBLAS(print_epoch_stats=False)\n",
    ")\n",
    "pipeline_blas.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_blas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "b = ax.bar([1,2], [2.52, 2.13], yerr=[134e-3, 94.5e-3], width=0.4)\n",
    "ax.bar_label(b, labels=['Baseline', 'BLAS'], padding=5)\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
