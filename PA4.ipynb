{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "\n",
    "Celio Bueri, Christoph Stelz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise question\n",
    "\n",
    "In the second example, the data are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SVCImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # initialise the number of iteration\n",
    "        t = 0\n",
    "\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # update the weights\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * y * x\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w\n",
    "\n",
    "            # print every 100 iterations\n",
    "            if self.print_epoch_stats and t % 100 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "       # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # initialise the number of iteration\n",
    "        t = 0\n",
    "\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # update the weights\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * x * y / (1 + np.exp(y*score))\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w\n",
    "\n",
    "            # print every 100 iterations\n",
    "            if self.print_epoch_stats and t % 100 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for logistic regression\n",
    "            loss += np.log(1 + np.exp(-y * x.dot(self.w)))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our classifiers \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 100, train_loss : 20.454155352283077\n",
      "iteration : 200, train_loss : 10.431490720950851\n",
      "iteration : 300, train_loss : 6.818815696479206\n",
      "iteration : 400, train_loss : 4.40849791060487\n",
      "iteration : 500, train_loss : 4.164731388062037\n",
      "iteration : 600, train_loss : 3.7108198508133743\n",
      "iteration : 700, train_loss : 2.8170523789582584\n",
      "iteration : 800, train_loss : 2.303689920960192\n",
      "iteration : 900, train_loss : 2.1328214538292976\n",
      "iteration : 1000, train_loss : 1.906895219785747\n",
      "iteration : 1100, train_loss : 1.7414459620558334\n",
      "iteration : 1200, train_loss : 1.635298974446465\n",
      "iteration : 1300, train_loss : 1.8574726852928007\n",
      "iteration : 1400, train_loss : 1.3179279607869085\n",
      "iteration : 1500, train_loss : 2.0276983399439183\n",
      "iteration : 1600, train_loss : 1.2217751730785256\n",
      "iteration : 1700, train_loss : 1.1787546584697646\n",
      "iteration : 1800, train_loss : 1.0680671939058548\n",
      "iteration : 1900, train_loss : 1.1481482256408766\n",
      "iteration : 2000, train_loss : 0.9634632174545488\n",
      "iteration : 2100, train_loss : 0.9618700725064584\n",
      "iteration : 2200, train_loss : 1.058236406604305\n",
      "iteration : 2300, train_loss : 0.9034800421629483\n",
      "iteration : 2400, train_loss : 1.1280760280537476\n",
      "iteration : 2500, train_loss : 1.0263416175253364\n",
      "iteration : 2600, train_loss : 0.918657441125688\n",
      "iteration : 2700, train_loss : 0.8281095135145671\n",
      "iteration : 2800, train_loss : 0.8180275332048026\n",
      "iteration : 2900, train_loss : 0.7819677193109237\n",
      "iteration : 3000, train_loss : 0.7657500211872676\n",
      "iteration : 3100, train_loss : 0.7328573815002655\n",
      "iteration : 3200, train_loss : 0.7312764541312363\n",
      "iteration : 3300, train_loss : 0.75279118971105\n",
      "iteration : 3400, train_loss : 0.7394394566778697\n",
      "iteration : 3500, train_loss : 0.832328017316747\n",
      "iteration : 3600, train_loss : 0.7189855124889319\n",
      "iteration : 3700, train_loss : 0.8731671445631881\n",
      "iteration : 3800, train_loss : 0.7164341305841527\n",
      "iteration : 3900, train_loss : 0.7330950745772183\n",
      "iteration : 4000, train_loss : 0.7862939340499792\n",
      "iteration : 4100, train_loss : 0.6691528594347242\n",
      "iteration : 4200, train_loss : 0.6822658904256295\n",
      "iteration : 4300, train_loss : 0.6422399864565461\n",
      "iteration : 4400, train_loss : 0.6378663767553079\n",
      "iteration : 4500, train_loss : 0.6110232995393776\n",
      "iteration : 4600, train_loss : 0.6201968202051378\n",
      "iteration : 4700, train_loss : 0.681798763459075\n",
      "iteration : 4800, train_loss : 0.5674157439519826\n",
      "iteration : 4900, train_loss : 0.7413931397245175\n",
      "iteration : 5000, train_loss : 0.5762004866454101\n",
      "iteration : 5100, train_loss : 0.5616438535264856\n",
      "iteration : 5200, train_loss : 0.5662888060932293\n",
      "iteration : 5300, train_loss : 0.5678361811703327\n",
      "iteration : 5400, train_loss : 0.713862329042454\n",
      "iteration : 5500, train_loss : 0.6056721542244318\n",
      "iteration : 5600, train_loss : 0.5893500944850204\n",
      "iteration : 5700, train_loss : 0.5683666494884144\n",
      "iteration : 5800, train_loss : 0.6003815562948251\n",
      "iteration : 5900, train_loss : 0.5442308502742674\n",
      "iteration : 6000, train_loss : 0.6049379078472498\n",
      "iteration : 6100, train_loss : 0.5193062373101969\n",
      "iteration : 6200, train_loss : 0.528755853174462\n",
      "iteration : 6300, train_loss : 0.5608338458959904\n",
      "iteration : 6400, train_loss : 0.5140326819414864\n",
      "iteration : 6500, train_loss : 0.5082251477868616\n",
      "iteration : 6600, train_loss : 0.5539084681137417\n",
      "iteration : 6700, train_loss : 0.5093758216967883\n",
      "iteration : 6800, train_loss : 0.5874685272339366\n",
      "iteration : 6900, train_loss : 0.496855119342261\n",
      "iteration : 7000, train_loss : 0.48376826024407543\n",
      "iteration : 7100, train_loss : 0.4850995215901512\n",
      "iteration : 7200, train_loss : 0.49513440348234555\n",
      "iteration : 7300, train_loss : 0.6053917897708999\n",
      "iteration : 7400, train_loss : 0.4890619380670456\n",
      "iteration : 7500, train_loss : 0.5211894600352186\n",
      "iteration : 7600, train_loss : 0.49312729833940677\n",
      "iteration : 7700, train_loss : 0.4753871522914019\n",
      "iteration : 7800, train_loss : 0.5457971373491192\n",
      "iteration : 7900, train_loss : 0.4815508653392444\n",
      "iteration : 8000, train_loss : 0.47735945136875463\n",
      "iteration : 8100, train_loss : 0.47094803636684723\n",
      "iteration : 8200, train_loss : 0.5453954963046956\n",
      "iteration : 8300, train_loss : 0.48764748632356003\n",
      "iteration : 8400, train_loss : 0.4702685427057336\n",
      "iteration : 8500, train_loss : 0.49312455700639596\n",
      "iteration : 8600, train_loss : 0.5160779358180155\n",
      "iteration : 8700, train_loss : 0.48186867514278275\n",
      "iteration : 8800, train_loss : 0.5273044437549111\n",
      "iteration : 8900, train_loss : 0.472549770481003\n",
      "iteration : 9000, train_loss : 0.45892302818724495\n",
      "iteration : 9100, train_loss : 0.5269965420329091\n",
      "iteration : 9200, train_loss : 0.4929575863709392\n",
      "iteration : 9300, train_loss : 0.4863005071606952\n",
      "iteration : 9400, train_loss : 0.476280776318265\n",
      "iteration : 9500, train_loss : 0.5398969609889513\n",
      "iteration : 9600, train_loss : 0.4884607047658136\n",
      "iteration : 9700, train_loss : 0.45287244539159044\n",
      "iteration : 9800, train_loss : 0.4796985446209759\n",
      "iteration : 9900, train_loss : 0.46580121612161374\n",
      "iteration : 10000, train_loss : 0.45608315160860763\n",
      "iteration : 10100, train_loss : 0.45112927797724556\n",
      "iteration : 10200, train_loss : 0.4457336316240297\n",
      "iteration : 10300, train_loss : 0.467616307086405\n",
      "iteration : 10400, train_loss : 0.44505440188830425\n",
      "iteration : 10500, train_loss : 0.48124304691390585\n",
      "iteration : 10600, train_loss : 0.4800995160651169\n",
      "iteration : 10700, train_loss : 0.4405398039675127\n",
      "iteration : 10800, train_loss : 0.4794867565095458\n",
      "iteration : 10900, train_loss : 0.4541155283513172\n",
      "iteration : 11000, train_loss : 0.46200807923860965\n",
      "iteration : 11100, train_loss : 0.4372972956213632\n",
      "iteration : 11200, train_loss : 0.43543797124345024\n",
      "iteration : 11300, train_loss : 0.44735167122646524\n",
      "iteration : 11400, train_loss : 0.5029700348711106\n",
      "iteration : 11500, train_loss : 0.446070330732516\n",
      "iteration : 11600, train_loss : 0.5063255835836546\n",
      "iteration : 11700, train_loss : 0.4350454081453378\n",
      "iteration : 11800, train_loss : 0.4563611042039561\n",
      "iteration : 11900, train_loss : 0.42789477895299327\n",
      "iteration : 12000, train_loss : 0.432449323012963\n",
      "iteration : 12100, train_loss : 0.4344872935472712\n",
      "iteration : 12200, train_loss : 0.43061731337387854\n",
      "iteration : 12300, train_loss : 0.4307541783090514\n",
      "iteration : 12400, train_loss : 0.4299092853193362\n",
      "iteration : 12500, train_loss : 0.451798147317374\n",
      "iteration : 12600, train_loss : 0.4888391822831568\n",
      "iteration : 12700, train_loss : 0.42886544519102426\n",
      "iteration : 12800, train_loss : 0.42994414950719556\n",
      "iteration : 12900, train_loss : 0.431800033823505\n",
      "iteration : 13000, train_loss : 0.43631563040534127\n",
      "iteration : 13100, train_loss : 0.4248108695371006\n",
      "iteration : 13200, train_loss : 0.4194686827298436\n",
      "iteration : 13300, train_loss : 0.5945359879480868\n",
      "iteration : 13400, train_loss : 0.4184066986522443\n",
      "iteration : 13500, train_loss : 0.4191243984727936\n",
      "iteration : 13600, train_loss : 0.42248401225062904\n",
      "iteration : 13700, train_loss : 0.4165318749875002\n",
      "iteration : 13800, train_loss : 0.4288382995951697\n",
      "iteration : 13900, train_loss : 0.41374990574848014\n",
      "iteration : 14000, train_loss : 0.41427773021356473\n",
      "iteration : 14100, train_loss : 0.4287636209516335\n",
      "iteration : 14200, train_loss : 0.4359304347625757\n",
      "iteration : 14300, train_loss : 0.42148715225906647\n",
      "iteration : 14400, train_loss : 0.41357851342131224\n",
      "iteration : 14500, train_loss : 0.43408683501632184\n",
      "iteration : 14600, train_loss : 0.4077861734719463\n",
      "iteration : 14700, train_loss : 0.4536370872644187\n",
      "iteration : 14800, train_loss : 0.41173326393713827\n",
      "iteration : 14900, train_loss : 0.4144032688818703\n",
      "iteration : 15000, train_loss : 0.4060282112481646\n",
      "iteration : 15100, train_loss : 0.5740501514032451\n",
      "iteration : 15200, train_loss : 0.42288656618892767\n",
      "iteration : 15300, train_loss : 0.41413131507646306\n",
      "iteration : 15400, train_loss : 0.4292447218267032\n",
      "iteration : 15500, train_loss : 0.41038344296232443\n",
      "iteration : 15600, train_loss : 0.42352651521561596\n",
      "iteration : 15700, train_loss : 0.41582125560651806\n",
      "iteration : 15800, train_loss : 0.414912824745129\n",
      "iteration : 15900, train_loss : 0.435794499608197\n",
      "iteration : 16000, train_loss : 0.4251948583300127\n",
      "iteration : 16100, train_loss : 0.4114640304683277\n",
      "iteration : 16200, train_loss : 0.40890786076429064\n",
      "iteration : 16300, train_loss : 0.42828616930853436\n",
      "iteration : 16400, train_loss : 0.43327986192176476\n",
      "iteration : 16500, train_loss : 0.434560893930937\n",
      "iteration : 16600, train_loss : 0.42616872518768983\n",
      "iteration : 16700, train_loss : 0.41347494593115225\n",
      "iteration : 16800, train_loss : 0.3995375629390014\n",
      "iteration : 16900, train_loss : 0.4005416863604533\n",
      "iteration : 17000, train_loss : 0.42261011854461994\n",
      "iteration : 17100, train_loss : 0.41909362546065637\n",
      "iteration : 17200, train_loss : 0.43583197097212784\n",
      "iteration : 17300, train_loss : 0.39899380061110323\n",
      "iteration : 17400, train_loss : 0.398912287228972\n",
      "iteration : 17500, train_loss : 0.4433116785328647\n",
      "iteration : 17600, train_loss : 0.4003938138636896\n",
      "iteration : 17700, train_loss : 0.3999355944829968\n",
      "iteration : 17800, train_loss : 0.39759641621454955\n",
      "iteration : 17900, train_loss : 0.40135907755846073\n",
      "iteration : 18000, train_loss : 0.4101286340466062\n",
      "iteration : 18100, train_loss : 0.434011463899336\n",
      "iteration : 18200, train_loss : 0.4005910864574901\n",
      "iteration : 18300, train_loss : 0.4072101460924724\n",
      "iteration : 18400, train_loss : 0.39657987911236386\n",
      "iteration : 18500, train_loss : 0.3975791945932841\n",
      "iteration : 18600, train_loss : 0.39899193620307016\n",
      "iteration : 18700, train_loss : 0.4082945615360352\n",
      "iteration : 18800, train_loss : 0.41817905436156955\n",
      "iteration : 18900, train_loss : 0.4038218784596136\n",
      "iteration : 19000, train_loss : 0.39630006562591\n",
      "iteration : 19100, train_loss : 0.4293717896265714\n",
      "iteration : 19200, train_loss : 0.39770256467498916\n",
      "iteration : 19300, train_loss : 0.39906462695784567\n",
      "iteration : 19400, train_loss : 0.47998537399495655\n",
      "iteration : 19500, train_loss : 0.40508569643750064\n",
      "iteration : 19600, train_loss : 0.4083318272438134\n",
      "iteration : 19700, train_loss : 0.40199841330116626\n",
      "iteration : 19800, train_loss : 0.39477406168478113\n",
      "iteration : 19900, train_loss : 0.3964146544549628\n",
      "iteration : 20000, train_loss : 0.4014746611414305\n",
      "Training time: 4.85 sec.\n",
      "Accuracy: 0.8141.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(n_iter=20000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 100, train_loss : 18.346923850221152\n",
      "iteration : 200, train_loss : 8.44844584187435\n",
      "iteration : 300, train_loss : 4.86279527564301\n",
      "iteration : 400, train_loss : 3.6352474925896194\n",
      "iteration : 500, train_loss : 2.6931446324417507\n",
      "iteration : 600, train_loss : 2.26152931898637\n",
      "iteration : 700, train_loss : 1.9140066962530726\n",
      "iteration : 800, train_loss : 2.198708082267002\n",
      "iteration : 900, train_loss : 1.5500902416490088\n",
      "iteration : 1000, train_loss : 1.3836073838723995\n",
      "iteration : 1100, train_loss : 1.1853112669976391\n",
      "iteration : 1200, train_loss : 1.0763617650094912\n",
      "iteration : 1300, train_loss : 0.984622888067199\n",
      "iteration : 1400, train_loss : 0.9117174227934777\n",
      "iteration : 1500, train_loss : 0.9158955806064727\n",
      "iteration : 1600, train_loss : 1.287333627716432\n",
      "iteration : 1700, train_loss : 0.7609371246055905\n",
      "iteration : 1800, train_loss : 0.8182874408301428\n",
      "iteration : 1900, train_loss : 0.6992799443791388\n",
      "iteration : 2000, train_loss : 0.667702762794453\n",
      "iteration : 2100, train_loss : 0.6595638122220805\n",
      "iteration : 2200, train_loss : 0.6786923806861227\n",
      "iteration : 2300, train_loss : 0.6269730091051515\n",
      "iteration : 2400, train_loss : 0.613826050840745\n",
      "iteration : 2500, train_loss : 0.6227951309318784\n",
      "iteration : 2600, train_loss : 0.5763160663341609\n",
      "iteration : 2700, train_loss : 0.5445979293450528\n",
      "iteration : 2800, train_loss : 0.6161698648231706\n",
      "iteration : 2900, train_loss : 0.5676238975681829\n",
      "iteration : 3000, train_loss : 0.5299427060188812\n",
      "iteration : 3100, train_loss : 0.5265097076920967\n",
      "iteration : 3200, train_loss : 0.5169990097564009\n",
      "iteration : 3300, train_loss : 0.5094987597835108\n",
      "iteration : 3400, train_loss : 0.5379630018454535\n",
      "iteration : 3500, train_loss : 0.5091857031840749\n",
      "iteration : 3600, train_loss : 0.4999216271204153\n",
      "iteration : 3700, train_loss : 0.48339407120230715\n",
      "iteration : 3800, train_loss : 0.5114392135242537\n",
      "iteration : 3900, train_loss : 0.47811452354077716\n",
      "iteration : 4000, train_loss : 0.4927631550675782\n",
      "iteration : 4100, train_loss : 0.47907152785771234\n",
      "iteration : 4200, train_loss : 0.47849332611481465\n",
      "iteration : 4300, train_loss : 0.49304142334364076\n",
      "iteration : 4400, train_loss : 0.4588708844116294\n",
      "iteration : 4500, train_loss : 0.46384620107979413\n",
      "iteration : 4600, train_loss : 0.44863251966003403\n",
      "iteration : 4700, train_loss : 0.45092709530992314\n",
      "iteration : 4800, train_loss : 0.4528181654392973\n",
      "iteration : 4900, train_loss : 0.44784165222221245\n",
      "iteration : 5000, train_loss : 0.44762075130486484\n",
      "iteration : 5100, train_loss : 0.4519344853073014\n",
      "iteration : 5200, train_loss : 0.44573400149435316\n",
      "iteration : 5300, train_loss : 0.43904756568450265\n",
      "iteration : 5400, train_loss : 0.44150822929178046\n",
      "iteration : 5500, train_loss : 0.44423724963706934\n",
      "iteration : 5600, train_loss : 0.43822452423623004\n",
      "iteration : 5700, train_loss : 0.4556103136365621\n",
      "iteration : 5800, train_loss : 0.45713604215009185\n",
      "iteration : 5900, train_loss : 0.4347797643939492\n",
      "iteration : 6000, train_loss : 0.4516390813533726\n",
      "iteration : 6100, train_loss : 0.45005011093740593\n",
      "iteration : 6200, train_loss : 0.43838516517757325\n",
      "iteration : 6300, train_loss : 0.4368216387286761\n",
      "iteration : 6400, train_loss : 0.44497136685915273\n",
      "iteration : 6500, train_loss : 0.44317910011517225\n",
      "iteration : 6600, train_loss : 0.45296572938440194\n",
      "iteration : 6700, train_loss : 0.45798994947348104\n",
      "iteration : 6800, train_loss : 0.44051808981441654\n",
      "iteration : 6900, train_loss : 0.43414082870185833\n",
      "iteration : 7000, train_loss : 0.4490673275610517\n",
      "iteration : 7100, train_loss : 0.42838283971030094\n",
      "iteration : 7200, train_loss : 0.42937548386726027\n",
      "iteration : 7300, train_loss : 0.43181419027298873\n",
      "iteration : 7400, train_loss : 0.43324398249325036\n",
      "iteration : 7500, train_loss : 0.43516366057628136\n",
      "iteration : 7600, train_loss : 0.43669771295859305\n",
      "iteration : 7700, train_loss : 0.44278814688593093\n",
      "iteration : 7800, train_loss : 0.4498917366325281\n",
      "iteration : 7900, train_loss : 0.4410357545194371\n",
      "iteration : 8000, train_loss : 0.44012699777768716\n",
      "iteration : 8100, train_loss : 0.4459572806791353\n",
      "iteration : 8200, train_loss : 0.4421268392237545\n",
      "iteration : 8300, train_loss : 0.45271326890478564\n",
      "iteration : 8400, train_loss : 0.43922297073693506\n",
      "iteration : 8500, train_loss : 0.4341285891153317\n",
      "iteration : 8600, train_loss : 0.450493221436173\n",
      "iteration : 8700, train_loss : 0.43624023294436465\n",
      "iteration : 8800, train_loss : 0.4379092036874074\n",
      "iteration : 8900, train_loss : 0.45684159660213186\n",
      "iteration : 9000, train_loss : 0.43824521206965644\n",
      "iteration : 9100, train_loss : 0.4400419289394448\n",
      "iteration : 9200, train_loss : 0.4426981055700927\n",
      "iteration : 9300, train_loss : 0.4358668525239469\n",
      "iteration : 9400, train_loss : 0.4435205662297695\n",
      "iteration : 9500, train_loss : 0.430935390507746\n",
      "iteration : 9600, train_loss : 0.4294595267362573\n",
      "iteration : 9700, train_loss : 0.43172577702613446\n",
      "iteration : 9800, train_loss : 0.4298960957660639\n",
      "iteration : 9900, train_loss : 0.4322758192680894\n",
      "iteration : 10000, train_loss : 0.4316941126894589\n",
      "iteration : 10100, train_loss : 0.43900460099181365\n",
      "iteration : 10200, train_loss : 0.4296978334178437\n",
      "iteration : 10300, train_loss : 0.42994421922800596\n",
      "iteration : 10400, train_loss : 0.4272333995431312\n",
      "iteration : 10500, train_loss : 0.4308121142060711\n",
      "iteration : 10600, train_loss : 0.42878097499258666\n",
      "iteration : 10700, train_loss : 0.4272296368843653\n",
      "iteration : 10800, train_loss : 0.4284790153342548\n",
      "iteration : 10900, train_loss : 0.4315784012635145\n",
      "iteration : 11000, train_loss : 0.43108690622535134\n",
      "iteration : 11100, train_loss : 0.4343706368672664\n",
      "iteration : 11200, train_loss : 0.43015422472566667\n",
      "iteration : 11300, train_loss : 0.4332850307208607\n",
      "iteration : 11400, train_loss : 0.4338415974991656\n",
      "iteration : 11500, train_loss : 0.43570954051252614\n",
      "iteration : 11600, train_loss : 0.44520601586924546\n",
      "iteration : 11700, train_loss : 0.429938767897933\n",
      "iteration : 11800, train_loss : 0.4399994585036752\n",
      "iteration : 11900, train_loss : 0.429415311239798\n",
      "iteration : 12000, train_loss : 0.43223262659158124\n",
      "iteration : 12100, train_loss : 0.4329017434667727\n",
      "iteration : 12200, train_loss : 0.43229951859788324\n",
      "iteration : 12300, train_loss : 0.4305164457924392\n",
      "iteration : 12400, train_loss : 0.4320212821151264\n",
      "iteration : 12500, train_loss : 0.4326748927348427\n",
      "iteration : 12600, train_loss : 0.4309643043093138\n",
      "iteration : 12700, train_loss : 0.43009623866160424\n",
      "iteration : 12800, train_loss : 0.42963088131356164\n",
      "iteration : 12900, train_loss : 0.43840185866548326\n",
      "iteration : 13000, train_loss : 0.4311019914034039\n",
      "iteration : 13100, train_loss : 0.44323023558171293\n",
      "iteration : 13200, train_loss : 0.43583926235376125\n",
      "iteration : 13300, train_loss : 0.4355829649948669\n",
      "iteration : 13400, train_loss : 0.42935054783646287\n",
      "iteration : 13500, train_loss : 0.4296580626282313\n",
      "iteration : 13600, train_loss : 0.42892260342405636\n",
      "iteration : 13700, train_loss : 0.43035088577518554\n",
      "iteration : 13800, train_loss : 0.42827373574255145\n",
      "iteration : 13900, train_loss : 0.42741486535948303\n",
      "iteration : 14000, train_loss : 0.4277682181589217\n",
      "iteration : 14100, train_loss : 0.42649798441393233\n",
      "iteration : 14200, train_loss : 0.433977603414859\n",
      "iteration : 14300, train_loss : 0.429321456405339\n",
      "iteration : 14400, train_loss : 0.43413013985877413\n",
      "iteration : 14500, train_loss : 0.4280049105041986\n",
      "iteration : 14600, train_loss : 0.42714295765824273\n",
      "iteration : 14700, train_loss : 0.42955734355431474\n",
      "iteration : 14800, train_loss : 0.4281273729592142\n",
      "iteration : 14900, train_loss : 0.4314842901252627\n",
      "iteration : 15000, train_loss : 0.44476668960231047\n",
      "iteration : 15100, train_loss : 0.4260739338120364\n",
      "iteration : 15200, train_loss : 0.43490287221202384\n",
      "iteration : 15300, train_loss : 0.4273917929035528\n",
      "iteration : 15400, train_loss : 0.42703126432663585\n",
      "iteration : 15500, train_loss : 0.4257813989323977\n",
      "iteration : 15600, train_loss : 0.4342370091689004\n",
      "iteration : 15700, train_loss : 0.43000901917755807\n",
      "iteration : 15800, train_loss : 0.4299364544407665\n",
      "iteration : 15900, train_loss : 0.44171823917105085\n",
      "iteration : 16000, train_loss : 0.42857750274122475\n",
      "iteration : 16100, train_loss : 0.4314496027629477\n",
      "iteration : 16200, train_loss : 0.43093584200045676\n",
      "iteration : 16300, train_loss : 0.4308866847848733\n",
      "iteration : 16400, train_loss : 0.4280956451500176\n",
      "iteration : 16500, train_loss : 0.43664762805744917\n",
      "iteration : 16600, train_loss : 0.4325419266900847\n",
      "iteration : 16700, train_loss : 0.4308169699649539\n",
      "iteration : 16800, train_loss : 0.4344065659887503\n",
      "iteration : 16900, train_loss : 0.4293007785010328\n",
      "iteration : 17000, train_loss : 0.42956248171167133\n",
      "iteration : 17100, train_loss : 0.4287100242700893\n",
      "iteration : 17200, train_loss : 0.4251385872267279\n",
      "iteration : 17300, train_loss : 0.4332940957984859\n",
      "iteration : 17400, train_loss : 0.434846144540945\n",
      "iteration : 17500, train_loss : 0.42927876462578746\n",
      "iteration : 17600, train_loss : 0.4257563997459368\n",
      "iteration : 17700, train_loss : 0.4284256899323698\n",
      "iteration : 17800, train_loss : 0.4281476631093678\n",
      "iteration : 17900, train_loss : 0.42998265294885496\n",
      "iteration : 18000, train_loss : 0.4281553801832938\n",
      "iteration : 18100, train_loss : 0.4370098659982304\n",
      "iteration : 18200, train_loss : 0.43355935918884586\n",
      "iteration : 18300, train_loss : 0.4334204349031525\n",
      "iteration : 18400, train_loss : 0.4340398776958169\n",
      "iteration : 18500, train_loss : 0.427942525647678\n",
      "iteration : 18600, train_loss : 0.4387687326766828\n",
      "iteration : 18700, train_loss : 0.4239773074484213\n",
      "iteration : 18800, train_loss : 0.4238523614984191\n",
      "iteration : 18900, train_loss : 0.42425399010830755\n",
      "iteration : 19000, train_loss : 0.42614427111919007\n",
      "iteration : 19100, train_loss : 0.4232308920258246\n",
      "iteration : 19200, train_loss : 0.43333728734555915\n",
      "iteration : 19300, train_loss : 0.4269853356405908\n",
      "iteration : 19400, train_loss : 0.42494802847604657\n",
      "iteration : 19500, train_loss : 0.42175827566497787\n",
      "iteration : 19600, train_loss : 0.4257994482510576\n",
      "iteration : 19700, train_loss : 0.4230117232024365\n",
      "iteration : 19800, train_loss : 0.42473426945532977\n",
      "iteration : 19900, train_loss : 0.4245177298002716\n",
      "iteration : 20000, train_loss : 0.42334176871498874\n",
      "Training time: 7.22 sec.\n",
      "Accuracy: 0.8271.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    LogisticRegressionImpl(n_iter=20000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (loss=-0.453):\n",
      "{'classifier__n_iter': 100000, 'classifier__regularizer': 0.0001}\n",
      "Accuracy: 0.8758.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_iter': [10000, 30000, 100000],\n",
    "    'classifier__regularizer': [0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"vect\",TfidfVectorizer()),\n",
    "    (\"select\", SelectKBest(k=1000)),\n",
    "    (\"norm\", Normalizer()),\n",
    "    (\"classifier\", SVCImpl(print_epoch_stats=False))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameter (loss=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "Yguess = grid_search.best_estimator_.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "\n",
    "### a) BLAS operations\n",
    "\n",
    "In this version we replaced the Numpy operations with BLAS functions from scipy.\n",
    "One pitfall we encountered is that with inplace operations like `daxpy`, the argument ordering matters a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy\n",
    "\n",
    "class SVCImplBLAS(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = ddot(self.w, x)\n",
    "                \n",
    "                dscal(1 - self.regularizer * self.lr, self.w)\n",
    "                if y * score < 1:\n",
    "                    daxpy(x, self.w, a=(self.lr * y))\n",
    "                \n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(ddot(self.w, self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "outputs": [],
   "source": [
    "\n",
    "class SparseSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = np.dot(self.w[x.indices], x.data)\n",
    "\n",
    "            # regularizer\n",
    "            self.w *= (1 - self.regularizer * lr)\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                self.w[x.indices] += (lr * y) * x.data\n",
    "\n",
    "            if self.print_epoch_stats and t % 1000 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2.0 * min_weight + loss / X.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1000, train_loss : [16.09393676]\n",
      "iteration : 2000, train_loss : [7.09094877]\n",
      "iteration : 3000, train_loss : [4.40177564]\n",
      "iteration : 4000, train_loss : [3.26213348]\n",
      "iteration : 5000, train_loss : [2.36717034]\n",
      "iteration : 6000, train_loss : [1.94579162]\n",
      "iteration : 7000, train_loss : [1.52642049]\n",
      "iteration : 8000, train_loss : [1.29761803]\n",
      "iteration : 9000, train_loss : [1.11232521]\n",
      "iteration : 10000, train_loss : [0.99499786]\n",
      "iteration : 11000, train_loss : [0.85978396]\n",
      "iteration : 12000, train_loss : [0.77275968]\n",
      "iteration : 13000, train_loss : [0.70568607]\n",
      "iteration : 14000, train_loss : [0.64325771]\n",
      "iteration : 15000, train_loss : [0.58329278]\n",
      "iteration : 16000, train_loss : [0.54327596]\n",
      "iteration : 17000, train_loss : [0.49974744]\n",
      "iteration : 18000, train_loss : [0.46316966]\n",
      "iteration : 19000, train_loss : [0.43181738]\n",
      "iteration : 20000, train_loss : [0.41370066]\n",
      "iteration : 21000, train_loss : [0.39274772]\n",
      "iteration : 22000, train_loss : [0.37197366]\n",
      "iteration : 23000, train_loss : [0.35597037]\n",
      "iteration : 24000, train_loss : [0.34225307]\n",
      "iteration : 25000, train_loss : [0.32746985]\n",
      "iteration : 26000, train_loss : [0.32216669]\n",
      "iteration : 27000, train_loss : [0.30919218]\n",
      "iteration : 28000, train_loss : [0.30094723]\n",
      "iteration : 29000, train_loss : [0.30253906]\n",
      "iteration : 30000, train_loss : [0.29120666]\n",
      "iteration : 31000, train_loss : [0.28208669]\n",
      "iteration : 32000, train_loss : [0.28026464]\n",
      "iteration : 33000, train_loss : [0.2816302]\n",
      "iteration : 34000, train_loss : [0.26938091]\n",
      "iteration : 35000, train_loss : [0.26555281]\n",
      "iteration : 36000, train_loss : [0.26630124]\n",
      "iteration : 37000, train_loss : [0.26626484]\n",
      "iteration : 38000, train_loss : [0.26557418]\n",
      "iteration : 39000, train_loss : [0.26732983]\n",
      "iteration : 40000, train_loss : [0.26896507]\n",
      "Training time: 27.68 sec.\n",
      "Accuracy: 0.8489.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    SparseSVC(n_iter=40000, regularizer=0.00001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Faster scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w, a):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return a * np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparseSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        # pick indices randomly\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # list of instances picked randomly\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        # initialize vector scaling\n",
    "        a = 1\n",
    "        #  number of iterations\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "            # update learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = sparse_dense_dot(x, self.w, a)\n",
    "\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                add_sparse_to_dense(x, self.w, (lr * y / a))\n",
    "\n",
    "            # update vector scaling\n",
    "            # We verify that a is positive because for large number of iteration\n",
    "            # a could be rounded as 0.0 and cause an error\n",
    "            if (1 - self.regularizer * lr) * a > 0.0:\n",
    "                a = (1 - self.regularizer * lr) * a\n",
    "\n",
    "        self.w =  a * self.w\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 5.83 sec.\n",
      "Accuracy: 0.8531.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    SparseSVC(n_iter=100000, regularizer=0.00001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline):\n",
    "    pipeline.fit(Xtrain, Ytrain) # make sure its fitted\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.52 s ± 134 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_baseline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(print_epoch_stats=False)\n",
    ")\n",
    "pipeline_baseline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13 s ± 94.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pipeline_blas = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImplBLAS(print_epoch_stats=False)\n",
    ")\n",
    "pipeline_blas.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_blas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "b = ax.bar([1,2], [2.52, 2.13], yerr=[134e-3, 94.5e-3], width=0.8)\n",
    "ax.bar_label(b, labels=['Baseline', 'BLAS'], padding=5)\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
