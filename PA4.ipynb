{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "\n",
    "Celio Bueri, Christoph Stelz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise question\n",
    "\n",
    "In the second example, the data are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SVCImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # initialise the number of iteration\n",
    "        t = 0\n",
    "\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # update the weights\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * y * x\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w\n",
    "\n",
    "            # print every 100 iterations\n",
    "            if self.print_epoch_stats and t % 100 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "       # pick random indexes for each iteration\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # initialise the number of iteration\n",
    "        t = 0\n",
    "\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "            # update the learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            # update the weights\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * x * y / (1 + np.exp(y*score))\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w\n",
    "\n",
    "            # print every 100 iterations\n",
    "            if self.print_epoch_stats and t % 100 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for logistic regression\n",
    "            loss += np.log(1 + np.exp(-y * x.dot(self.w)))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our classifiers \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 100, train_loss : 23.7500901400999\n",
      "iteration : 200, train_loss : 10.35161614333661\n",
      "iteration : 300, train_loss : 6.282353450834151\n",
      "iteration : 400, train_loss : 4.643914602593124\n",
      "iteration : 500, train_loss : 3.7376008689449387\n",
      "iteration : 600, train_loss : 3.0790032622289316\n",
      "iteration : 700, train_loss : 2.6288736308419764\n",
      "iteration : 800, train_loss : 2.3461505790043704\n",
      "iteration : 900, train_loss : 2.090620285691015\n",
      "iteration : 1000, train_loss : 2.143810997129464\n",
      "iteration : 1100, train_loss : 1.8283827745517633\n",
      "iteration : 1200, train_loss : 1.6750045692332953\n",
      "iteration : 1300, train_loss : 1.5602801599243854\n",
      "iteration : 1400, train_loss : 1.4328170149479367\n",
      "iteration : 1500, train_loss : 1.3204937776901113\n",
      "iteration : 1600, train_loss : 1.237390641348171\n",
      "iteration : 1700, train_loss : 1.4611643209183096\n",
      "iteration : 1800, train_loss : 1.5929343320402545\n",
      "iteration : 1900, train_loss : 1.0559429107252298\n",
      "iteration : 2000, train_loss : 1.0379414856660045\n",
      "iteration : 2100, train_loss : 1.1544089142185296\n",
      "iteration : 2200, train_loss : 1.0531117519804085\n",
      "iteration : 2300, train_loss : 0.9797990939217351\n",
      "iteration : 2400, train_loss : 0.8970557478092895\n",
      "iteration : 2500, train_loss : 0.922930615997613\n",
      "iteration : 2600, train_loss : 0.9592992722821579\n",
      "iteration : 2700, train_loss : 0.9034618991934484\n",
      "iteration : 2800, train_loss : 0.8274327301065351\n",
      "iteration : 2900, train_loss : 1.0542600832598388\n",
      "iteration : 3000, train_loss : 0.8549530619548457\n",
      "iteration : 3100, train_loss : 0.7912835874354849\n",
      "iteration : 3200, train_loss : 0.7383306896423962\n",
      "iteration : 3300, train_loss : 0.8358864610516632\n",
      "iteration : 3400, train_loss : 0.7569352232296479\n",
      "iteration : 3500, train_loss : 0.7745750944365674\n",
      "iteration : 3600, train_loss : 0.7254645187163986\n",
      "iteration : 3700, train_loss : 0.6886351560911419\n",
      "iteration : 3800, train_loss : 0.6610123875429454\n",
      "iteration : 3900, train_loss : 0.7584466375494433\n",
      "iteration : 4000, train_loss : 0.7054212163421498\n",
      "iteration : 4100, train_loss : 0.6572557246447658\n",
      "iteration : 4200, train_loss : 0.6760313278870649\n",
      "iteration : 4300, train_loss : 0.6709476412122435\n",
      "iteration : 4400, train_loss : 0.6273058533639583\n",
      "iteration : 4500, train_loss : 0.6471132921638452\n",
      "iteration : 4600, train_loss : 0.9574310431776765\n",
      "iteration : 4700, train_loss : 0.6107015207910842\n",
      "iteration : 4800, train_loss : 0.5961164569217605\n",
      "iteration : 4900, train_loss : 0.5783024632794368\n",
      "iteration : 5000, train_loss : 0.603638759177496\n",
      "iteration : 5100, train_loss : 0.5889503671780449\n",
      "iteration : 5200, train_loss : 0.5725080043372954\n",
      "iteration : 5300, train_loss : 0.5602086774280607\n",
      "iteration : 5400, train_loss : 0.5467972011029314\n",
      "iteration : 5500, train_loss : 0.540520595802982\n",
      "iteration : 5600, train_loss : 0.5994717768309611\n",
      "iteration : 5700, train_loss : 0.5733733499121613\n",
      "iteration : 5800, train_loss : 0.5246496035786163\n",
      "iteration : 5900, train_loss : 0.5340579089452047\n",
      "iteration : 6000, train_loss : 0.5861305749943024\n",
      "iteration : 6100, train_loss : 0.535359381158048\n",
      "iteration : 6200, train_loss : 0.5268721073854674\n",
      "iteration : 6300, train_loss : 0.553099423257596\n",
      "iteration : 6400, train_loss : 0.5098439805569372\n",
      "iteration : 6500, train_loss : 0.5474663464923322\n",
      "iteration : 6600, train_loss : 0.5179160712173664\n",
      "iteration : 6700, train_loss : 0.5540703918000385\n",
      "iteration : 6800, train_loss : 0.5141830147981428\n",
      "iteration : 6900, train_loss : 0.5128614385973717\n",
      "iteration : 7000, train_loss : 0.5408733856407613\n",
      "iteration : 7100, train_loss : 0.5163539883781374\n",
      "iteration : 7200, train_loss : 0.533384635892667\n",
      "iteration : 7300, train_loss : 0.5030856759522104\n",
      "iteration : 7400, train_loss : 0.5110729433272743\n",
      "iteration : 7500, train_loss : 0.6137912175084597\n",
      "iteration : 7600, train_loss : 0.48317700077592324\n",
      "iteration : 7700, train_loss : 0.4890685663867682\n",
      "iteration : 7800, train_loss : 0.5159662590410398\n",
      "iteration : 7900, train_loss : 0.47987293631000016\n",
      "iteration : 8000, train_loss : 0.47712913419389996\n",
      "iteration : 8100, train_loss : 0.55153535714125\n",
      "iteration : 8200, train_loss : 0.48361961274339327\n",
      "iteration : 8300, train_loss : 0.46429578035627506\n",
      "iteration : 8400, train_loss : 0.4556898550822201\n",
      "iteration : 8500, train_loss : 0.49747526291211513\n",
      "iteration : 8600, train_loss : 0.48199505777901896\n",
      "iteration : 8700, train_loss : 0.4713799571587156\n",
      "iteration : 8800, train_loss : 0.49951475271682744\n",
      "iteration : 8900, train_loss : 0.4502732147887353\n",
      "iteration : 9000, train_loss : 0.4530358281176792\n",
      "iteration : 9100, train_loss : 0.47473131603186325\n",
      "iteration : 9200, train_loss : 0.4497971849678485\n",
      "iteration : 9300, train_loss : 0.5252445993577773\n",
      "iteration : 9400, train_loss : 0.45332026008381354\n",
      "iteration : 9500, train_loss : 0.4547775361658938\n",
      "iteration : 9600, train_loss : 0.46735344973932774\n",
      "iteration : 9700, train_loss : 0.45039962849500964\n",
      "iteration : 9800, train_loss : 0.5526035031959029\n",
      "iteration : 9900, train_loss : 0.4534560171812496\n",
      "iteration : 10000, train_loss : 0.45597916268011135\n",
      "Training time: 2.88 sec.\n",
      "Accuracy: 0.8170.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(n_iter=10000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 100, train_loss : 23.606718607669603\n",
      "iteration : 200, train_loss : 9.348465207494986\n",
      "iteration : 300, train_loss : 5.251044064534685\n",
      "iteration : 400, train_loss : 3.5134494604180837\n",
      "iteration : 500, train_loss : 2.8933426378757474\n",
      "iteration : 600, train_loss : 2.35824765003992\n",
      "iteration : 700, train_loss : 2.233111749533012\n",
      "iteration : 800, train_loss : 1.7019273786512907\n",
      "iteration : 900, train_loss : 1.536981202057833\n",
      "iteration : 1000, train_loss : 1.528208786866879\n",
      "iteration : 1100, train_loss : 1.2237996601134773\n",
      "iteration : 1200, train_loss : 1.172943697660891\n",
      "iteration : 1300, train_loss : 1.0348530123254376\n",
      "iteration : 1400, train_loss : 0.9702088284902379\n",
      "iteration : 1500, train_loss : 1.0299184915737136\n",
      "iteration : 1600, train_loss : 0.8372265766871312\n",
      "iteration : 1700, train_loss : 0.9127504500794765\n",
      "iteration : 1800, train_loss : 0.8757333143706965\n",
      "iteration : 1900, train_loss : 0.7474620209967601\n",
      "iteration : 2000, train_loss : 0.7238424370453336\n",
      "iteration : 2100, train_loss : 0.6413891033593686\n",
      "iteration : 2200, train_loss : 0.6395435253941787\n",
      "iteration : 2300, train_loss : 0.6262289187013688\n",
      "iteration : 2400, train_loss : 0.5929620748100126\n",
      "iteration : 2500, train_loss : 0.590779520604661\n",
      "iteration : 2600, train_loss : 0.5836634111110446\n",
      "iteration : 2700, train_loss : 0.5678832988887712\n",
      "iteration : 2800, train_loss : 0.5733892252881707\n",
      "iteration : 2900, train_loss : 0.5857825458294221\n",
      "iteration : 3000, train_loss : 0.5292468762720007\n",
      "iteration : 3100, train_loss : 0.520514904266712\n",
      "iteration : 3200, train_loss : 0.5108307433073518\n",
      "iteration : 3300, train_loss : 0.5112564880326743\n",
      "iteration : 3400, train_loss : 0.4996948235908797\n",
      "iteration : 3500, train_loss : 0.528032046773932\n",
      "iteration : 3600, train_loss : 0.49726294061917314\n",
      "iteration : 3700, train_loss : 0.5110691095021827\n",
      "iteration : 3800, train_loss : 0.4880762922427986\n",
      "iteration : 3900, train_loss : 0.48514768607381825\n",
      "iteration : 4000, train_loss : 0.47367220512920116\n",
      "iteration : 4100, train_loss : 0.47013140805158016\n",
      "iteration : 4200, train_loss : 0.4983552519144191\n",
      "iteration : 4300, train_loss : 0.46597351146494753\n",
      "iteration : 4400, train_loss : 0.46734516453875813\n",
      "iteration : 4500, train_loss : 0.4666847416201807\n",
      "iteration : 4600, train_loss : 0.46305729639533355\n",
      "iteration : 4700, train_loss : 0.4536950171473788\n",
      "iteration : 4800, train_loss : 0.4574511826826219\n",
      "iteration : 4900, train_loss : 0.46117440620813255\n",
      "iteration : 5000, train_loss : 0.45182816342574417\n",
      "iteration : 5100, train_loss : 0.47829267062953773\n",
      "iteration : 5200, train_loss : 0.4864030720796261\n",
      "iteration : 5300, train_loss : 0.4469821143411469\n",
      "iteration : 5400, train_loss : 0.4411090807685122\n",
      "iteration : 5500, train_loss : 0.44377606019837423\n",
      "iteration : 5600, train_loss : 0.44516718536485184\n",
      "iteration : 5700, train_loss : 0.45524853110593816\n",
      "iteration : 5800, train_loss : 0.44028835307030956\n",
      "iteration : 5900, train_loss : 0.43725198291361644\n",
      "iteration : 6000, train_loss : 0.44057616782861364\n",
      "iteration : 6100, train_loss : 0.4356456603901817\n",
      "iteration : 6200, train_loss : 0.4511266659830769\n",
      "iteration : 6300, train_loss : 0.4355290039736241\n",
      "iteration : 6400, train_loss : 0.43448176092187085\n",
      "iteration : 6500, train_loss : 0.435832037633091\n",
      "iteration : 6600, train_loss : 0.4451263986820149\n",
      "iteration : 6700, train_loss : 0.4429957850136807\n",
      "iteration : 6800, train_loss : 0.4366664449261679\n",
      "iteration : 6900, train_loss : 0.4528218721242038\n",
      "iteration : 7000, train_loss : 0.44644074728232525\n",
      "iteration : 7100, train_loss : 0.4412769337749929\n",
      "iteration : 7200, train_loss : 0.4411063312640694\n",
      "iteration : 7300, train_loss : 0.4364238691015707\n",
      "iteration : 7400, train_loss : 0.45998214328732434\n",
      "iteration : 7500, train_loss : 0.4724018134601655\n",
      "iteration : 7600, train_loss : 0.4407379670085355\n",
      "iteration : 7700, train_loss : 0.44756765278894656\n",
      "iteration : 7800, train_loss : 0.4429600409411627\n",
      "iteration : 7900, train_loss : 0.44454617366381827\n",
      "iteration : 8000, train_loss : 0.4504503960126778\n",
      "iteration : 8100, train_loss : 0.4562217903367141\n",
      "iteration : 8200, train_loss : 0.44683497664850935\n",
      "iteration : 8300, train_loss : 0.44391508341121483\n",
      "iteration : 8400, train_loss : 0.4400233836893769\n",
      "iteration : 8500, train_loss : 0.444535669527144\n",
      "iteration : 8600, train_loss : 0.4404221765926519\n",
      "iteration : 8700, train_loss : 0.4735464610597352\n",
      "iteration : 8800, train_loss : 0.43428541054674985\n",
      "iteration : 8900, train_loss : 0.4389903863210462\n",
      "iteration : 9000, train_loss : 0.4355406776166269\n",
      "iteration : 9100, train_loss : 0.43550537083341423\n",
      "iteration : 9200, train_loss : 0.4543799431139658\n",
      "iteration : 9300, train_loss : 0.43286908579641603\n",
      "iteration : 9400, train_loss : 0.42462318357942636\n",
      "iteration : 9500, train_loss : 0.42513303926555124\n",
      "iteration : 9600, train_loss : 0.42708144104355367\n",
      "iteration : 9700, train_loss : 0.43061191964794543\n",
      "iteration : 9800, train_loss : 0.44176820265592176\n",
      "iteration : 9900, train_loss : 0.4268103919448245\n",
      "iteration : 10000, train_loss : 0.42363545326123053\n",
      "Training time: 3.59 sec.\n",
      "Accuracy: 0.8204.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    LogisticRegressionImpl(n_iter=10000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter (loss=-0.424):\n",
      "{'classifier__n_iter': 1000000, 'classifier__regularizer': 1e-05}\n",
      "Accuracy: 0.8976.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_iter': [10000, 30000, 100000, 1000000],\n",
    "    'classifier__regularizer': [0.01, 0.001, 0.0001, 0.00001]\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"vect\",TfidfVectorizer()),\n",
    "    (\"select\", SelectKBest(k=1000)),\n",
    "    (\"norm\", Normalizer()),\n",
    "    (\"classifier\", SVCImpl(print_epoch_stats=False))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameter (loss=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)\n",
    "\n",
    "Yguess = grid_search.best_estimator_.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "\n",
    "### a) BLAS operations\n",
    "\n",
    "In this version we replaced the Numpy operations with BLAS functions from scipy.\n",
    "One pitfall we encountered is that with inplace operations like `daxpy`, the argument ordering matters a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy\n",
    "\n",
    "class SVCImplBLAS(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = ddot(self.w, x)\n",
    "                \n",
    "                dscal(1 - self.regularizer * self.lr, self.w)\n",
    "                if y * score < 1:\n",
    "                    daxpy(x, self.w, a=(self.lr * y))\n",
    "                \n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(ddot(self.w, self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [],
   "source": [
    "\n",
    "class SVCImplSparse(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = np.dot(self.w[x.indices], x.data)\n",
    "\n",
    "            # regularizer\n",
    "            self.w *= (1 - self.regularizer * lr)\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                self.w[x.indices] += (lr * y) * x.data\n",
    "\n",
    "            if self.print_epoch_stats and t % 1000 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2.0 * min_weight + loss / X.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Faster scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w, a):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return a * np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SVCImplSparseScaling(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        # pick indices randomly\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        # list of instances picked randomly\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        # initialize vector scaling\n",
    "        a = 1\n",
    "        #  number of iterations\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "            # update learning rate\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = sparse_dense_dot(x, self.w, a)\n",
    "\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                add_sparse_to_dense(x, self.w, (lr * y / a))\n",
    "\n",
    "            # update vector scaling\n",
    "            # We verify that a is positive because for large number of iteration\n",
    "            # a could be rounded as 0.0 and cause an error\n",
    "            if (1 - self.regularizer * lr) * a > 0.0:\n",
    "                a = (1 - self.regularizer * lr) * a\n",
    "\n",
    "        self.w =  a * self.w\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline):\n",
    "    pipeline.fit(Xtrain, Ytrain) # make sure its fitted\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "pipeline_baseline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(n_iter=100000,regularizer=0.0001,print_epoch_stats=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.23 s ± 48.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_baseline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8443.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "pipeline_blas = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImplBLAS(print_epoch_stats=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.46 s ± 57.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_blas.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_blas)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Sparse vector"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [],
   "source": [
    "pipeline_sparse = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    SVCImplSparse(n_iter=100000,regularizer=0.0001,print_epoch_stats=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.8 s ± 61.7 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_sparse.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8561.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_sparse)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Speeding up the scaling operation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [],
   "source": [
    "pipeline_sparse_scaling = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "    SVCImplSparseScaling(n_iter=100000,regularizer=0.0001,print_epoch_stats=False)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.63 s ± 88.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_sparse_scaling.fit(Xtrain, Ytrain)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8582.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_sparse_scaling)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "b = ax.bar([1,2], [2.52, 2.13], yerr=[134e-3, 94.5e-3], width=0.8)\n",
    "ax.bar_label(b, labels=['Baseline', 'BLAS'], padding=5)\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
