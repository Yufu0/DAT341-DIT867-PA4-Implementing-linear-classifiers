{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Programming Assignment 4\n",
    "\n",
    "Celio Bueri, Christoph Stelz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise question\n",
    "\n",
    "In the second example, the data are not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boilerplate code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class LinearClassifier(BaseEstimator):\n",
    "    \"\"\"\n",
    "    General class for binary linear classifiers. Implements the predict\n",
    "    function, which is the same for all binary linear classifiers. There are\n",
    "    also two utility functions.\n",
    "    \"\"\"\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Computes the decision function for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "        return X.dot(self.w)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predicts the outputs for the inputs X. The inputs are assumed to be\n",
    "        stored in a matrix, where each row contains the features for one\n",
    "        instance.\n",
    "        \"\"\"\n",
    "\n",
    "        # First compute the output scores\n",
    "        scores = self.decision_function(X)\n",
    "\n",
    "        # Select the positive or negative class label, depending on whether\n",
    "        # the score was positive or negative.\n",
    "        out = np.select([scores >= 0.0, scores < 0.0],\n",
    "                        [self.positive_class,\n",
    "                         self.negative_class])\n",
    "        return out\n",
    "\n",
    "    def find_classes(self, Y):\n",
    "        \"\"\"\n",
    "        Finds the set of output classes in the output part Y of the training set.\n",
    "        If there are exactly two classes, one of them is associated to positive\n",
    "        classifier scores, the other one to negative scores. If the number of\n",
    "        classes is not 2, an error is raised.\n",
    "        \"\"\"\n",
    "        classes = sorted(set(Y))\n",
    "        if len(classes) != 2:\n",
    "            raise Exception(\"this does not seem to be a 2-class problem\")\n",
    "        self.positive_class = classes[1]\n",
    "        self.negative_class = classes[0]\n",
    "\n",
    "    def encode_outputs(self, Y):\n",
    "        \"\"\"\n",
    "        A helper function that converts all outputs to +1 or -1.\n",
    "        \"\"\"\n",
    "        return np.array([1 if y == self.positive_class else -1 for y in Y])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVC Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class SVCImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        indices = np.random.randint(X.shape[0], size=self.n_iter)\n",
    "\n",
    "        t = 0\n",
    "        for i in indices:\n",
    "            t += 1\n",
    "\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = X[i], Ye[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = x.dot(self.w)\n",
    "\n",
    "            if y * score < 1:\n",
    "                self.w = (1 - self.regularizer * lr) * self.w + lr * y * x\n",
    "            else:\n",
    "                self.w = (1 - self.regularizer * self.lr) * self.w\n",
    "\n",
    "            if self.print_epoch_stats and t % 100 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LogisticRegressionImpl(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, probability=False, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.probability = probability\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "       # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "                score = y * x.dot(self.w)\n",
    "                self.w = (1 - self.regularizer * self.lr) * self.w + self.lr * y * self.sigmoid(-score) * x\n",
    "\n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for logistic regression\n",
    "            loss += float(np.log(1 + np.exp(-y * x.dot(self.w))))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Preparing training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import time\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def read_data(corpus_file):\n",
    "    X = []\n",
    "    Y = []\n",
    "    with open(corpus_file, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            _, y, _, x = line.split(maxsplit=3)\n",
    "            X.append(x.strip())\n",
    "            Y.append(y)\n",
    "    return X, Y\n",
    "\n",
    "\n",
    "# Read all the documents.\n",
    "X, Y = read_data('data/all_sentiment_shuffled.txt')\n",
    "\n",
    "# Split into training and test parts.\n",
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing our classifiers \t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 100, train_loss : 15.13428184811562\n",
      "iteration : 200, train_loss : 8.744016828111363\n",
      "iteration : 300, train_loss : 6.768464656447211\n",
      "iteration : 400, train_loss : 5.531080971520089\n",
      "iteration : 500, train_loss : 4.411445258614702\n",
      "iteration : 600, train_loss : 3.923597682657262\n",
      "iteration : 700, train_loss : 3.5054965811144476\n",
      "iteration : 800, train_loss : 3.1219238352018284\n",
      "iteration : 900, train_loss : 2.7884684354737233\n",
      "iteration : 1000, train_loss : 2.550242790730322\n",
      "iteration : 1100, train_loss : 2.358274367402297\n",
      "iteration : 1200, train_loss : 2.21420985376166\n",
      "iteration : 1300, train_loss : 2.0675918813457352\n",
      "iteration : 1400, train_loss : 2.049127257652016\n",
      "iteration : 1500, train_loss : 1.873211834217693\n",
      "iteration : 1600, train_loss : 1.7753704323372206\n",
      "iteration : 1700, train_loss : 1.7098193149832182\n",
      "iteration : 1800, train_loss : 1.642994775704135\n",
      "iteration : 1900, train_loss : 1.6672279265678074\n",
      "iteration : 2000, train_loss : 1.5405542587359136\n",
      "iteration : 2100, train_loss : 1.469160088781524\n",
      "iteration : 2200, train_loss : 1.4216787207345833\n",
      "iteration : 2300, train_loss : 1.3841546253418937\n",
      "iteration : 2400, train_loss : 1.3550998500082265\n",
      "iteration : 2500, train_loss : 1.3137167192275157\n",
      "iteration : 2600, train_loss : 1.2834591963530992\n",
      "iteration : 2700, train_loss : 1.2440342712390202\n",
      "iteration : 2800, train_loss : 1.2080123757666998\n",
      "iteration : 2900, train_loss : 1.1965938623596444\n",
      "iteration : 3000, train_loss : 1.1528464551665638\n",
      "iteration : 3100, train_loss : 1.1342100946175766\n",
      "iteration : 3200, train_loss : 1.11182298831793\n",
      "iteration : 3300, train_loss : 1.1109054165852532\n",
      "iteration : 3400, train_loss : 1.0682208746934447\n",
      "iteration : 3500, train_loss : 1.051804689033102\n",
      "iteration : 3600, train_loss : 1.0376184413895302\n",
      "iteration : 3700, train_loss : 1.0219387117688916\n",
      "iteration : 3800, train_loss : 1.0115862318445956\n",
      "iteration : 3900, train_loss : 1.0079974800431273\n",
      "iteration : 4000, train_loss : 0.9969221495188355\n",
      "iteration : 4100, train_loss : 0.9635283739569855\n",
      "iteration : 4200, train_loss : 0.9522540682986094\n",
      "iteration : 4300, train_loss : 0.9499270245782228\n",
      "iteration : 4400, train_loss : 0.9370102298424301\n",
      "iteration : 4500, train_loss : 0.9151172918210382\n",
      "iteration : 4600, train_loss : 0.906938053246185\n",
      "iteration : 4700, train_loss : 0.9011146450989265\n",
      "iteration : 4800, train_loss : 0.8907711052240948\n",
      "iteration : 4900, train_loss : 0.8848822221312083\n",
      "iteration : 5000, train_loss : 0.876289593504714\n",
      "iteration : 5100, train_loss : 0.8593495579296804\n",
      "iteration : 5200, train_loss : 0.8535364089799786\n",
      "iteration : 5300, train_loss : 0.8511377170926325\n",
      "iteration : 5400, train_loss : 0.8394122193690623\n",
      "iteration : 5500, train_loss : 0.8386908332836065\n",
      "iteration : 5600, train_loss : 0.8265847726081572\n",
      "iteration : 5700, train_loss : 0.8225694941192631\n",
      "iteration : 5800, train_loss : 0.8186063782165732\n",
      "iteration : 5900, train_loss : 0.8136108396229569\n",
      "iteration : 6000, train_loss : 0.8044287264000882\n",
      "iteration : 6100, train_loss : 0.8097550843245522\n",
      "iteration : 6200, train_loss : 0.7942324809972314\n",
      "iteration : 6300, train_loss : 0.7843227295102163\n",
      "iteration : 6400, train_loss : 0.8020234023003121\n",
      "iteration : 6500, train_loss : 0.7886962751022268\n",
      "iteration : 6600, train_loss : 0.7687893010269979\n",
      "iteration : 6700, train_loss : 0.763795640065857\n",
      "iteration : 6800, train_loss : 0.7582810736260204\n",
      "iteration : 6900, train_loss : 0.7551417008661361\n",
      "iteration : 7000, train_loss : 0.752168742441991\n",
      "iteration : 7100, train_loss : 0.7625284637094254\n",
      "iteration : 7200, train_loss : 0.7429583909796608\n",
      "iteration : 7300, train_loss : 0.7401123241592786\n",
      "iteration : 7400, train_loss : 0.7423266835653273\n",
      "iteration : 7500, train_loss : 0.7349282954548187\n",
      "iteration : 7600, train_loss : 0.7301861332077452\n",
      "iteration : 7700, train_loss : 0.7261005003682002\n",
      "iteration : 7800, train_loss : 0.7273667240612036\n",
      "iteration : 7900, train_loss : 0.7452704579862857\n",
      "iteration : 8000, train_loss : 0.7275236021321089\n",
      "iteration : 8100, train_loss : 0.7148258871250219\n",
      "iteration : 8200, train_loss : 0.7095204211962962\n",
      "iteration : 8300, train_loss : 0.7061051740863526\n",
      "iteration : 8400, train_loss : 0.7053146914237602\n",
      "iteration : 8500, train_loss : 0.7053612891524863\n",
      "iteration : 8600, train_loss : 0.698651243394933\n",
      "iteration : 8700, train_loss : 0.6993266090210868\n",
      "iteration : 8800, train_loss : 0.6937864925202654\n",
      "iteration : 8900, train_loss : 0.6936526899524931\n",
      "iteration : 9000, train_loss : 0.6898988670679693\n",
      "iteration : 9100, train_loss : 0.6902529634070276\n",
      "iteration : 9200, train_loss : 0.6888922375176749\n",
      "iteration : 9300, train_loss : 0.6850319423225086\n",
      "iteration : 9400, train_loss : 0.6834914946443877\n",
      "iteration : 9500, train_loss : 0.6874435222680404\n",
      "iteration : 9600, train_loss : 0.6788229670716408\n",
      "iteration : 9700, train_loss : 0.6776442063761012\n",
      "iteration : 9800, train_loss : 0.6883075019797824\n",
      "iteration : 9900, train_loss : 0.673428091406659\n",
      "iteration : 10000, train_loss : 0.6716840238599084\n",
      "Training time: 3.03 sec.\n",
      "Accuracy: 0.8187.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(n_iter=10000, regularizer=0.001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, train_loss : 0.6288857169774531\n",
      "epoch : 1, train_loss : 0.5840018940929901\n",
      "epoch : 2, train_loss : 0.5511767701367489\n",
      "epoch : 3, train_loss : 0.5261244113878396\n",
      "epoch : 4, train_loss : 0.5062936920256184\n",
      "epoch : 5, train_loss : 0.49011851663793615\n",
      "epoch : 6, train_loss : 0.4765995710252395\n",
      "epoch : 7, train_loss : 0.4650742894372502\n",
      "epoch : 8, train_loss : 0.45508760073903537\n",
      "epoch : 9, train_loss : 0.4463169192656121\n",
      "epoch : 10, train_loss : 0.4385270914760717\n",
      "epoch : 11, train_loss : 0.43154242018454747\n",
      "epoch : 12, train_loss : 0.4252287470781953\n",
      "epoch : 13, train_loss : 0.41948164728788945\n",
      "epoch : 14, train_loss : 0.4142184477857359\n",
      "epoch : 15, train_loss : 0.4093727030957251\n",
      "epoch : 16, train_loss : 0.4048902894709579\n",
      "epoch : 17, train_loss : 0.40072658932183824\n",
      "epoch : 18, train_loss : 0.3968444253813822\n",
      "epoch : 19, train_loss : 0.3932125202979789\n",
      "Training time: 4.23 sec.\n",
      "Accuracy: 0.8175.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our LogisticRegression implementation\n",
    "    LogisticRegressionImpl()\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "param_grid = {\n",
    "    'classifier__n_iter': [10, 30, 10],\n",
    "    'classifier__regularizer': [0.1, 1, 2]\n",
    "}\n",
    "\n",
    "\n",
    "pipeline = Pipeline(steps=[\n",
    "    (\"vect\",TfidfVectorizer()),\n",
    "    (\"select\", SelectKBest(k=1000)),\n",
    "    (\"norm\", Normalizer()),\n",
    "    (\"classifier\", SVCImpl(print_epoch_stats=False))\n",
    "])\n",
    "\n",
    "grid_search = GridSearchCV(pipeline, param_grid, n_jobs=-1, return_train_score=True)\n",
    "\n",
    "grid_search.fit(X, Y)\n",
    "\n",
    "print(\"Best parameter (loss=%0.3f):\" % grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing Performance\n",
    "\n",
    "### a) BLAS operations\n",
    "\n",
    "In this version we replaced the Numpy operations with BLAS functions from scipy.\n",
    "One pitfall we encountered is that with inplace operations like `daxpy`, the argument ordering matters a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy.linalg.blas import ddot, dscal, daxpy\n",
    "\n",
    "class SVCImplBLAS(LinearClassifier):\n",
    "    def __init__(self, n_iter=20, lr=0.01, regularizer=0.0, print_epoch_stats=True):\n",
    "        \"\"\"\n",
    "        The constructor can optionally take a parameter n_iter specifying how\n",
    "        many times we want to iterate through the training set.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.lr = lr\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"\n",
    "        Train a linear classifier using the perceptron learning algorithm.\n",
    "        \"\"\"\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # Convert all outputs to +1 (for the positive class) or -1 (negative).\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # If necessary, convert the sparse matrix returned by a vectorizer\n",
    "        # into a normal NumPy matrix.\n",
    "        if not isinstance(X, np.ndarray):\n",
    "            X = X.toarray()\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        n_features = X.shape[1]\n",
    "        self.w = np.zeros(n_features)\n",
    "\n",
    "        # training algorithm:\n",
    "        for i in range(self.n_iter):\n",
    "            for x, y in zip(X, Ye):\n",
    "\n",
    "                # Compute the output score for this instance.\n",
    "                score = ddot(self.w, x)\n",
    "                \n",
    "                dscal(1 - self.regularizer * self.lr, self.w)\n",
    "                if y * score < 1:\n",
    "                    daxpy(x, self.w, a=(self.lr * y))\n",
    "                \n",
    "            if self.print_epoch_stats:\n",
    "                print(f'epoch : {i}, train_loss : {self.score(X, Y)}')\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(ddot(self.w, self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return self.regularizer / 2 * min_weight + loss / X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b) Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparseSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = sparse_dense_dot(x, self.w)\n",
    "\n",
    "            (1 - self.regularizer * lr) * self.w + lr * y * x\n",
    "\n",
    "            # regularizer\n",
    "            self.w *= (1 - self.regularizer * lr)\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                add_sparse_to_dense(x, self.w, lr * y)\n",
    "\n",
    "            if self.print_epoch_stats and t % 1000 == 0:\n",
    "                print(f'iteration : {t}, train_loss : { - self.score(X, Y)}')\n",
    "\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2.0 * min_weight + loss / X.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1000, train_loss : [2.03824451]\n",
      "iteration : 2000, train_loss : [1.19830306]\n",
      "iteration : 3000, train_loss : [0.97008858]\n",
      "iteration : 4000, train_loss : [0.87433692]\n",
      "iteration : 5000, train_loss : [0.81274848]\n",
      "iteration : 6000, train_loss : [0.7683792]\n",
      "iteration : 7000, train_loss : [0.75054165]\n",
      "iteration : 8000, train_loss : [0.74559668]\n",
      "iteration : 9000, train_loss : [0.74120014]\n",
      "iteration : 10000, train_loss : [0.74267376]\n",
      "Training time: 40.75 sec.\n",
      "Accuracy: 0.8116.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our LogisticRegression implementation\n",
    "    SparseSVC(n_iter=10000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c) Faster scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "outputs": [],
   "source": [
    "def add_sparse_to_dense(x, w, factor):\n",
    "    \"\"\"\n",
    "    Adds a sparse vector x, scaled by some factor, to a dense vector.\n",
    "    This can be seen as the equivalent of w += factor * x when x is a dense\n",
    "    vector.\n",
    "    \"\"\"\n",
    "    w[x.indices] += factor * x.data\n",
    "\n",
    "def sparse_dense_dot(x, w, a):\n",
    "    \"\"\"\n",
    "    Computes the dot product between a sparse vector x and a dense vector w.\n",
    "    \"\"\"\n",
    "    return a * np.dot(w[x.indices], x.data)\n",
    "\n",
    "\n",
    "class SparseSVC(LinearClassifier):\n",
    "\n",
    "    def __init__(self, n_iter=1000, regularizer=0.001, print_epoch_stats=True):\n",
    "        self.n_iter = n_iter\n",
    "        self.regularizer = regularizer\n",
    "        self.print_epoch_stats = print_epoch_stats\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        self.find_classes(Y)\n",
    "\n",
    "        # First determine which output class will be associated with positive\n",
    "        # and negative scores, respectively.\n",
    "        Ye = self.encode_outputs(Y)\n",
    "\n",
    "        # Initialize the weight vector to all zeros.\n",
    "        self.w = np.zeros(X.shape[1])\n",
    "\n",
    "        # Iteration through sparse matrices can be a bit slow, so we first\n",
    "        # prepare this list to speed up iteration.\n",
    "\n",
    "        indices = np.random.randint(1,X.shape[0], size=self.n_iter)\n",
    "\n",
    "        XY = list(zip(X[indices], Ye[indices]))\n",
    "\n",
    "        # initialize vector scaling\n",
    "        a = 1\n",
    "\n",
    "        t = 0\n",
    "        for i in range(self.n_iter):\n",
    "            t += 1\n",
    "\n",
    "            lr = 1.0 / (self.regularizer * t)\n",
    "\n",
    "            x, y = XY[i]\n",
    "\n",
    "            # Compute the output score for this instance.\n",
    "            score = sparse_dense_dot(x, self.w, a)\n",
    "\n",
    "            # If there was an error, update the weights.\n",
    "            if y*score <= 0:\n",
    "                add_sparse_to_dense(x, self.w, (lr * y / a))\n",
    "\n",
    "            # update vector scaling\n",
    "            if (1 - self.regularizer * lr) * a > 0:\n",
    "                a = (1 - self.regularizer * lr) * a\n",
    "\n",
    "        self.w[x.indices] =  a * self.w[x.indices]\n",
    "\n",
    "    def score(self, X, Y):\n",
    "        loss = 0.0\n",
    "        min_weight = np.min(self.w.dot(self.w))\n",
    "        for x, y in zip(X, self.encode_outputs(Y)):\n",
    "            # loss for svc\n",
    "            loss += max(0.0, 1.0 - y * x.dot(self.w))\n",
    "        return -(self.regularizer / 2 * min_weight + loss / X.shape[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time: 6.16 sec.\n",
      "Accuracy: 0.8548.\n"
     ]
    }
   ],
   "source": [
    "# Set up the preprocessing steps and the classifier.\n",
    "pipeline = make_pipeline(\n",
    "    TfidfVectorizer(ngram_range=(1,2)),\n",
    "    Normalizer(),\n",
    "\n",
    "    # NB that this is our LogisticRegression implementation\n",
    "    SparseSVC(n_iter=100000, regularizer=0.0001)\n",
    ")\n",
    "\n",
    "# Train the classifier.\n",
    "t0 = time.time()\n",
    "pipeline.fit(Xtrain, Ytrain)\n",
    "t1 = time.time()\n",
    "print('Training time: {:.2f} sec.'.format(t1-t0))\n",
    "\n",
    "# Evaluate on the test set.\n",
    "Yguess = pipeline.predict(Xtest)\n",
    "print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_pipeline(pipeline):\n",
    "    pipeline.fit(Xtrain, Ytrain) # make sure its fitted\n",
    "    Yguess = pipeline.predict(Xtest)\n",
    "    print('Accuracy: {:.4f}.'.format(accuracy_score(Ytest, Yguess)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.52 s ± 134 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "pipeline_baseline = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImpl(print_epoch_stats=False)\n",
    ")\n",
    "pipeline_baseline.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_baseline)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BLAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13 s ± 94.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "pipeline_blas = make_pipeline(\n",
    "    TfidfVectorizer(),\n",
    "    SelectKBest(k=1000),\n",
    "    Normalizer(),\n",
    "    SVCImplBLAS(print_epoch_stats=False)\n",
    ")\n",
    "pipeline_blas.fit(Xtrain, Ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8363.\n"
     ]
    }
   ],
   "source": [
    "evaluate_pipeline(pipeline_blas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "b = ax.bar([1,2], [2.52, 2.13], yerr=[134e-3, 94.5e-3], width=0.8)\n",
    "ax.bar_label(b, labels=['Baseline', 'BLAS'], padding=5)\n",
    "ax.spines[['right', 'top', 'bottom']].set_visible(False)\n",
    "ax.get_xaxis().set_visible(False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
